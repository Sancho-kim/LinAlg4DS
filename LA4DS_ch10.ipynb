{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "LA4DS_ch10.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyNDVXbluRrXFWneURmz+MuS"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Practical Linear Algebra for Data Science\n",
    "## Mike X Cohen (sincxpress.com)\n",
    "### https://www.oreilly.com/library/view/practical-linear-algebra/9781098120603/\n",
    "\n",
    "#### Code for chapter 10"
   ],
   "metadata": {
    "id": "SbGFWGzkd44U"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pq-JEewsnKHm"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# null space\n",
    "from scipy.linalg import null_space\n",
    "\n",
    "import sympy as sym\n",
    "\n",
    "\n",
    "# NOTE: these lines define global figure properties used for publication.\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg') # display figures in vector format\n",
    "plt.rcParams.update({'font.size':14}) # set global font size"
   ],
   "metadata": {
    "id": "GVWZlfT-nThD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "MvATXzy6gxLF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "F8nKu5cQjyeq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## this code creates figure 2\n",
    "\n",
    "# data\n",
    "x = [ 1,2,3,4,5 ]\n",
    "y = [ 0,3,2,5,5 ]\n",
    "\n",
    "# model\n",
    "X = np.hstack((np.ones((5,1)),np.array(x,ndmin=2).T))\n",
    "yHat = X @ np.linalg.inv(X.T@X) @ X.T @ y\n",
    "\n",
    "# plot the data and predicted values\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(x,y,'ks',markersize=15,label='Observed data')\n",
    "plt.plot(x,yHat,'o-',color=[.6,.6,.6],linewidth=3,markersize=8,label='Predicted data')\n",
    "\n",
    "# plot the residuals (errors)\n",
    "for n,y,yHat in zip(x,y,yHat):\n",
    "  plt.plot([n,n],[y,yHat],'--',color=[.8,.8,.8],zorder=-10)\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('Figure_10_02.png',dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "NsQQmXVz2lDS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "mWvZfl5qjyhT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Example in fake data"
   ],
   "metadata": {
    "id": "0c2ueLP3uz5b"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "numcourses = [13,4,12,3,14,13,12,9,11,7,13,11,9,2,5,7,10,0,9,7]\n",
    "happiness  = [70,25,54,21,80,68,84,62,57,40,60,64,45,38,51,52,58,21,75,70]\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "plt.plot(numcourses,happiness,'ks',markersize=15)\n",
    "plt.xlabel('Number of courses taken')\n",
    "plt.ylabel('General life happiness')\n",
    "plt.xlim([-1,15])\n",
    "plt.ylim([0,100])\n",
    "plt.grid()\n",
    "plt.xticks(range(0,15,2))\n",
    "plt.savefig('Figure_10_03.png',dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "QbDG-cjnjc6Q"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Build a statistical model\n",
    "\n",
    "# design matrix as a column vector\n",
    "X = np.array(numcourses,ndmin=2).T\n",
    "print(X.shape)\n",
    "\n",
    "# fit the model using the left-inverse\n",
    "X_leftinv = np.linalg.inv(X.T@X) @ X.T\n",
    "\n",
    "# solve for the coefficients\n",
    "beta = X_leftinv @ happiness\n",
    "beta"
   ],
   "metadata": {
    "id": "7oG9rOjRjagr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# let's plot it!\n",
    "\n",
    "# predicted data\n",
    "pred_happiness = X@beta\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "# plot the data and predicted values\n",
    "plt.plot(numcourses,happiness,'ks',markersize=15)\n",
    "plt.plot(numcourses,pred_happiness,'o-',color=[.6,.6,.6],linewidth=3,markersize=8)\n",
    "\n",
    "# plot the residuals (errors)\n",
    "for n,y,yHat in zip(numcourses,happiness,pred_happiness):\n",
    "  plt.plot([n,n],[y,yHat],'--',color=[.8,.8,.8],zorder=-10)\n",
    "\n",
    "plt.xlabel('Number of courses taken')\n",
    "plt.ylabel('General life happiness')\n",
    "plt.xlim([-1,15])\n",
    "plt.ylim([0,100])\n",
    "plt.xticks(range(0,15,2))\n",
    "plt.legend(['Real data','Predicted data','Residual'])\n",
    "plt.title(f'SSE = {np.sum((pred_happiness-happiness)**2):.2f}')\n",
    "plt.savefig('Figure_10_04.png',dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "jxh-OgHUjaj1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Build a statistical model with an intercept\n",
    "\n",
    "# design matrix as a column vector\n",
    "X = np.hstack((np.ones((20,1)),np.array(numcourses,ndmin=2).T))\n",
    "print(X.shape)\n",
    "\n",
    "# fit the model using the left-inverse\n",
    "X_leftinv = np.linalg.inv(X.T@X) @ X.T\n",
    "\n",
    "# solve for the coefficients\n",
    "beta = X_leftinv @ happiness\n",
    "beta"
   ],
   "metadata": {
    "id": "74HCXEvvjam4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# let's plot it!\n",
    "\n",
    "# predicted data\n",
    "pred_happiness = X@beta\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "# plot the data and predicted values\n",
    "plt.plot(numcourses,happiness,'ks',markersize=15)\n",
    "plt.plot(numcourses,pred_happiness,'o-',color=[.6,.6,.6],linewidth=3,markersize=8)\n",
    "\n",
    "# plot the residuals (errors)\n",
    "for n,y,yHat in zip(numcourses,happiness,pred_happiness):\n",
    "  plt.plot([n,n],[y,yHat],'--',color=[.8,.8,.8],zorder=-10)\n",
    "\n",
    "plt.xlabel('Number of courses taken')\n",
    "plt.ylabel('General life happiness')\n",
    "plt.xlim([-1,15])\n",
    "plt.ylim([0,100])\n",
    "plt.xticks(range(0,15,2))\n",
    "plt.legend(['Real data','Predicted data','Residual'])\n",
    "plt.title(f'SSE = {np.sum((pred_happiness-happiness)**2):.2f}')\n",
    "plt.savefig('Figure_10_05.png',dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "jAjOzukzjapt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "7Cp04-Qw2k6T"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 1"
   ],
   "metadata": {
    "id": "kwU4W1d62lL5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# compute residual\n",
    "res = happiness-pred_happiness\n",
    "\n",
    "\n",
    "# should be zero + some error\n",
    "print('Dot product: ' + str(np.dot(pred_happiness,res)) )\n",
    "print('Correlation: ' + str(np.corrcoef(pred_happiness,res)[0,1]))\n",
    "print(' ')\n",
    "\n",
    "\n",
    "# show in a plot\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(res,pred_happiness,'ko',markersize=12)\n",
    "plt.xlabel('Residual error')\n",
    "plt.ylabel('Model-predicted values')\n",
    "plt.title(f'r = {np.corrcoef(pred_happiness,res)[0,1]:.20f}')\n",
    "plt.savefig('Figure_10_06.png',dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "t9QF60gi2m38"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# correlation is smaller because we're dividing by the vector norms, e.g.,\n",
    "np.linalg.norm(res)"
   ],
   "metadata": {
    "id": "jCXHdxB3m30Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "s3O1fA8zCUXr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 2"
   ],
   "metadata": {
    "id": "o3svhXLXm4bs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# the residual is orthogonal to the entire column space of the design matrix.\n",
    "\n",
    "# I demonstrated this by showing that the residuals vector is in the left-null space of the design matrix.\n",
    "# I did that by using scipy.linalg.null_space to find the left-null space, augmenting that null-space basis\n",
    "# matrix by the residuals vector, and showing that the null space and augmented null space have the same rank.\n",
    "\n",
    "\n",
    "# compute the null space (via scipy.linalg)\n",
    "nullspace = null_space(X.T)\n",
    "\n",
    "\n",
    "# augment the residuals\n",
    "nullspaceAugment = np.hstack( (nullspace,res.reshape(-1,1)) )\n",
    "\n",
    "\n",
    "# print their ranks\n",
    "print(f'dim(  N(X)    ) = {np.linalg.matrix_rank(nullspace)}')\n",
    "print(f'dim( [N(X)|r] ) = {np.linalg.matrix_rank(nullspaceAugment)}')"
   ],
   "metadata": {
    "id": "TS5yPkwj2m62"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "fI3dJcb42m-B"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 3"
   ],
   "metadata": {
    "id": "0bPcsugrjast"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### Uncomment these lines to use random data\n",
    "# random design matrix and data vector\n",
    "# M,N = 20,3\n",
    "# X = np.random.randn(M,N)\n",
    "# happiness = np.random.randn(M,1)\n",
    "\n",
    "\n",
    "# recreate the design matrix and solution via left-inverse\n",
    "X = np.hstack((np.ones((20,1)),np.array(numcourses,ndmin=2).T))\n",
    "beta1 = np.linalg.inv(X.T@X) @ X.T @ happiness\n",
    "\n",
    "\n",
    "# QR decomp\n",
    "Q,R = np.linalg.qr(X)\n",
    "\n",
    "# beta coefficients implemented as translation of the math\n",
    "beta2 = np.linalg.inv(R) @ (Q.T@happiness)\n",
    "\n",
    "# and using back-substitution via RREF\n",
    "# Q'y, but needs to be reshaped into a column vector\n",
    "tmp = (Q.T@happiness).reshape(-1,1)\n",
    "Raug = np.hstack( (R,tmp) ) # augment the matrix\n",
    "Raug_r = sym.Matrix(Raug).rref()[0] # this gets the matrix\n",
    "beta3 = np.array(Raug_r[:,-1]) # convert back to numpy\n",
    "\n",
    "\n",
    "print('Betas from left-inverse: ')\n",
    "print(np.round(beta1,3)), print(' ')\n",
    "\n",
    "print('Betas from QR with inv(R): ')\n",
    "print(np.round(beta2,3)), print(' ')\n",
    "\n",
    "print('Betas from QR with back-substitution: ')\n",
    "print(np.round(np.array(beta3.T).astype(float),3)) # transposed to facilitate visual inspection"
   ],
   "metadata": {
    "id": "LnO4XR0b2nAg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# show the matrices\n",
    "print('Matrix R:')\n",
    "print(np.round(R,3)) # note that it's upper-triangular (as you know!)\n",
    "\n",
    "print(' ')\n",
    "print(\"Matrix R|Q'y:\")\n",
    "print(np.round(Raug,3))\n",
    "\n",
    "print(' ')\n",
    "print(\"Matrix RREF(R|Q'y):\")\n",
    "print(np.round(np.array(Raug_r).astype(float),3)) # convert to numpy floats"
   ],
   "metadata": {
    "id": "Y_Do_j3d2nDP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "a1D5ATv1Qu9s"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 4"
   ],
   "metadata": {
    "id": "uThOWkG7JMLz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# happiness with outliers due to typos (oops!)\n",
    "happiness_oops1 = [170,25,54,21,80,68,84,62,57,40,60,64,45,38,51,52,58,21,75,70]\n",
    "happiness_oops2 = [70,25,54,21,80,68,84,62,57,40,60,64,45,38,51,52,58,21,75,170]\n",
    "\n",
    "\n",
    "# design matrix and its left-inverse (doesn't change with the data)\n",
    "X = np.hstack((np.ones((20,1)),np.array(numcourses,ndmin=2).T))\n",
    "X_leftinv = np.linalg.inv(X.T@X) @ X.T\n",
    "\n",
    "\n",
    "\n",
    "_,axs = plt.subplots(1,3,figsize=(16,5))\n",
    "\n",
    "for axi,y in zip(axs,[happiness,happiness_oops1,happiness_oops2]):\n",
    "\n",
    "  # compute the best-fit parameters\n",
    "  beta = X_leftinv @ y\n",
    "\n",
    "  # predicted data\n",
    "  pred_happiness = X@beta\n",
    "\n",
    "\n",
    "  # plot the data and predicted values\n",
    "  axi.plot(numcourses,y,'ks',markersize=15)\n",
    "  axi.plot(numcourses,pred_happiness,'o-',color=[.6,.6,.6],linewidth=3,markersize=8)\n",
    "\n",
    "  # plot the residuals (errors)\n",
    "  for n,y,yHat in zip(numcourses,y,pred_happiness):\n",
    "    axi.plot([n,n],[y,yHat],'--',color=[.8,.8,.8],zorder=-10)\n",
    "\n",
    "  # make the plot look nicer\n",
    "  axi.set(xlabel='Number of courses taken',ylabel='General life happiness',\n",
    "          xlim=[-1,15],ylim=[0,100],xticks=range(0,15,2))\n",
    "  axi.legend(['Real data','Predicted data','Residual'])\n",
    "  axi.set_title(f'SSE = {np.sum((pred_happiness-y)**2):.2f}')\n",
    "  \n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Figure_10_07.png',dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "oh4rHkG6JNqk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "ztcpxLKZJMQt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 5"
   ],
   "metadata": {
    "id": "-FGBU3kz2k9P"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# matrix size\n",
    "n = 6\n",
    "\n",
    "# some random \"design matrix\"\n",
    "X = np.random.randn(n,n)\n",
    "\n",
    "# the target matrix (identity)\n",
    "Y = np.eye(n)\n",
    "\n",
    "\n",
    "# find the best-fitting model one column at a time\n",
    "Xinv1 = np.zeros_like(X)\n",
    "\n",
    "for coli in range(n):\n",
    "  Xinv1[:,coli] = np.linalg.inv(X.T@X) @ X.T @ Y[:,coli]\n",
    "\n",
    "\n",
    "\n",
    "# repeat but without a loop\n",
    "Xinv2 = np.linalg.inv(X.T@X) @ X.T @ Y\n",
    "\n",
    "\n",
    "# and the inverse using inv()\n",
    "Xinv3 = np.linalg.inv(X)\n",
    "\n",
    "\n",
    "# visualize\n",
    "_,axs = plt.subplots(1,3,figsize=(10,6))\n",
    "\n",
    "# column-wise least-squares\n",
    "axs[0].imshow( Xinv1@X ,cmap='gray')\n",
    "axs[0].set_title('Via column-wise LS')\n",
    "\n",
    "# matrix-wise least-squares\n",
    "axs[1].imshow( Xinv2@X ,cmap='gray' )\n",
    "axs[1].set_title('Via matrix-wise LS')\n",
    "\n",
    "# inv()\n",
    "axs[2].imshow( Xinv3@X ,cmap='gray' )\n",
    "axs[2].set_title('Via inv()')\n",
    "\n",
    "\n",
    "# don't need the tick marks\n",
    "for a in axs: a.set(xticks=[],yticks=[])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Figure_10_08.png',dpi=300)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "ecx7hGZe2lAT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# show they are equivalent \n",
    "# Note the relatively large rounding errors when comparing to inv() -- the left-inverse\n",
    "#   least-squares method is not a numerically stable method!\n",
    "\n",
    "\n",
    "print(Xinv1-Xinv2)\n",
    "print(' ')\n",
    "\n",
    "print(Xinv1-Xinv3)\n",
    "print(' ')\n",
    "\n",
    "print(Xinv2-Xinv3)"
   ],
   "metadata": {
    "id": "_XhAQVqtHroi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "1TCcpX4TPgkN"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
